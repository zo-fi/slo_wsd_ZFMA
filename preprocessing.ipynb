{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bc4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code related to loading and filtering datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "random.seed(101)\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "wsd_path = \"wsd_data/slo_sense_1.1.csv\"\n",
    "sense = pd.read_csv(wsd_path)\n",
    "sense.word_index = sense.word_index.astype(int)\n",
    "\n",
    "def filter_impact(df, name):\n",
    "  print(name)\n",
    "  print(\"word sense number {}\".format(df.senseID.nunique()))\n",
    "  print(\"lemma number{}\".format(df.lemma.nunique()))\n",
    "  print(\"unique sents number {}\".format(df.sent.nunique()))\n",
    "  print(\"all sents number: {}\".format(df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7748941f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n",
      "število besednih pomenov 11069\n",
      "število lem 5604\n",
      "število unikatnih stavkov 196655\n",
      "skupno število primerov: (202240, 7)\n",
      "len_filter\n",
      "število besednih pomenov 11068\n",
      "število lem 5603\n",
      "število unikatnih stavkov 194649\n",
      "skupno število primerov: (200188, 7)\n",
      "mwu_filter\n",
      "število besednih pomenov 10331\n",
      "število lem 4927\n",
      "število unikatnih stavkov 193536\n",
      "skupno število primerov: (198789, 7)\n",
      "multitag_filter\n",
      "število besednih pomenov 10318\n",
      "število lem 4927\n",
      "število unikatnih stavkov 193388\n",
      "skupno število primerov: (198438, 7)\n",
      "two sense per lemma: True\n",
      "four examples per sense: True\n",
      "describe filtered df\n",
      "število besednih pomenov 4633\n",
      "število lem 1597\n",
      "število unikatnih stavkov 139445\n",
      "skupno število primerov: (139445, 7)\n"
     ]
    }
   ],
   "source": [
    "#general filtering\n",
    "filter_impact(sense, \"original\")\n",
    "filt = sense[sense[\"sent\"].apply(lambda x: len(x) <= 540)].copy() # 200 at 75%, std = 170\n",
    "filter_impact(filt, \"len_filter\")\n",
    "\n",
    "#Manual corrections\n",
    "filt.at[83899, 'instance'] = 'multimedijske' #wrong inflection\n",
    "drop_list = [113682, 13226, 23644, 42813, 49063, 49078, 105551, 113676, 128790, 149384,153792, 174480]\n",
    "filt = filt.drop(index=drop_list) #duplicate examples with differing indices.\n",
    "\n",
    "#MWU removal\n",
    "filt = filt[filt.instance.apply(lambda x: \" \" not in x)].copy()\n",
    "filt = filt[filt.lemma.apply(lambda x: \" \" not in x)].copy()\n",
    "filt = filt[~filt.pos.apply(lambda x: \" \" in x)] #spaces in pos tags\n",
    "filter_impact(filt, \"mwu_filter\")\n",
    "\n",
    "filt.pos = filt.pos.apply(lambda x: \"NOUN\" if x == \"PROPN\" else x)\n",
    "filt.pos = filt.pos.apply(lambda x: x if x in [\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"] else \"OTHR\")\n",
    "\n",
    "#examples with multiple tags for the same target lemma\n",
    "filt[\"temp_text\"] = filt.lemma + \" \" + filt.sent\n",
    "group_obj = filt.groupby(\"temp_text\", as_index = False).senseID.nunique()\n",
    "multi_tagged = group_obj[group_obj.senseID > 1]\n",
    "multi_tagged_list = set(multi_tagged.temp_text)\n",
    "multi_tagged_ind  = filt[filt[\"temp_text\"].apply(lambda x: x in multi_tagged_list)]\n",
    "filt = filt.drop(multi_tagged_ind.index)\n",
    "del filt['temp_text']\n",
    "filter_impact(filt, \"multitag_filter\")\n",
    "\n",
    "## Slossbert specific filtering\n",
    "def compliant(df):\n",
    "    at_least_two = df.groupby(\"lemma\").senseID.nunique().min() == 2\n",
    "    sent_num_cond = df.groupby(\"senseID\").sent.count().min() == 4\n",
    "    print(\"two sense per lemma: {}\".format(at_least_two))\n",
    "    print(\"four examples per sense: {}\".format(sent_num_cond))\n",
    "    filter_impact(df, \"describe filtered df\")\n",
    "\n",
    "def elim_single_senses(df):\n",
    "    lemma_condition = df.groupby(\"lemma\").senseID.nunique() >= 2\n",
    "    two_senses = [lemma for lemma, condition in zip(lemma_condition.index.values, lemma_condition.values) if condition]\n",
    "    return df[df.lemma.apply(lambda x: x in two_senses)]\n",
    "    #filt_2sense_per_lemma = test_soup.copy()\n",
    "\n",
    "def elim_too_few_senses(df): #!+ misleading name\n",
    "    sent_num_cond = df.groupby(\"senseID\").sent.count() >= 4\n",
    "    two_example = [sense for sense, condition in zip(sent_num_cond.index.values, sent_num_cond.values) if condition]\n",
    "    return df[df.senseID.apply(lambda x : x in two_example)]\n",
    "\n",
    "filt = elim_single_senses(filt)\n",
    "filt = elim_too_few_senses(filt)\n",
    "filt = elim_single_senses(filt)\n",
    "filt = elim_too_few_senses(filt)\n",
    "\n",
    "\n",
    "#Sentence formatting\n",
    "#strips punctuation & double spaces and does '-weak supervision (glossbert idea)\n",
    "#4-metil whatever cases are already removed in the previous step (Probably MWU removal)\n",
    "\n",
    "def strip_punctuation(text):\n",
    "    punctuation_cats = set(['Pc', 'Pd', 'Ps', 'Pe', 'Pi', 'Pf', 'Po'])\n",
    "    return ''.join(x for x in text\n",
    "                   if unicodedata.category(x) not in punctuation_cats)\n",
    "\n",
    "def mark_target_word(old_string, target_w):\n",
    "    new_string = old_string.strip(\" \")\n",
    "    new_string = re.sub(\"  \", \" \", new_string)\n",
    "    new_string = re.sub(target_w, \"'\" + target_w + \"'\", new_string)\n",
    "    return new_string\n",
    "\n",
    "filt[\"sent\"] = filt.apply(lambda x: strip_punctuation(x[\"sent\"]), axis=1)\n",
    "filt[\"sent\"] = filt.apply(lambda x: mark_target_word(x[\"sent\"], x[\"instance\"]), axis=1)\n",
    "\n",
    "#duplikati vstran\n",
    "filt.drop('old_index', axis=1, inplace=True)\n",
    "filt.drop_duplicates(inplace = True)\n",
    "\n",
    "clean_df = filt.copy()\n",
    "clean_df[\"sentID\"] = [i for i in range(len(clean_df))]\n",
    "compliant(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efc1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more POS fixes\n",
    "fixes = {\n",
    "    'cepljiv%0': \"ADJ\",\n",
    "    'cepljiv%1': \"ADJ\",\n",
    "    'dovolj%0': \"ADV\",\n",
    "    'dovolj%1': \"ADV\",\n",
    "    'edin%0': \"ADJ\",\n",
    "    'edin%1': \"ADJ\",\n",
    "    'kad%0': \"NOUN\",\n",
    "    'kad%1': \"NOUN\",\n",
    "    'mnogo%0': \"ADV\",\n",
    "    'mnogo%1': \"ADV\",\n",
    "    'mnogo%2': \"ADV\",\n",
    "    'obročast%0': \"ADJ\",\n",
    "    'obročast%1': \"ADJ\",\n",
    "    'pritlikav%0': \"ADJ\",\n",
    "    'pritlikav%1': \"ADJ\",\n",
    "    'pritlikav%3': \"ADJ\",\n",
    "    'vas%0': \"NOUN\",\n",
    "    'vas%1': \"NOUN\",\n",
    "    'vas%2': \"NOUN\",\n",
    "    'veliko%0': \"ADV\",\n",
    "    'veliko%1': \"ADV\",\n",
    "    'veliko%2': \"ADV\",\n",
    "    'zvezdaš%0': \"NOUN\",\n",
    "    'zvezdaš%1': \"NOUN\"\n",
    "}\n",
    "\n",
    "#def replace(senseID_in):\n",
    "#    if senseID_in in fixes.keys():\n",
    "        \n",
    "\n",
    "clean_df[\"pos\"] = clean_df.apply(lambda x: fixes[x.senseID] if x.senseID in fixes.keys() else x.pos, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eacdec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two sense per lemma: True\n",
      "two examples per sense: True\n",
      "two sense per lemma: True\n",
      "two examples per sense: True\n",
      "two sense per lemma: True\n",
      "two examples per sense: True\n",
      "train\n",
      "število besednih pomenov 4633\n",
      "število lem 1597\n",
      "število unikatnih stavkov 104316\n",
      "skupno število primerov: (104316, 7)\n",
      "val\n",
      "število besednih pomenov 1743\n",
      "število lem 691\n",
      "število unikatnih stavkov 6972\n",
      "skupno število primerov: (6972, 7)\n",
      "test\n",
      "število besednih pomenov 4633\n",
      "število lem 1597\n",
      "število unikatnih stavkov 28157\n",
      "skupno število primerov: (28157, 7)\n"
     ]
    }
   ],
   "source": [
    "#splits\n",
    "\n",
    "#test set\n",
    "# at least 2 and max 8 examples per word sense (Stratified sample with a cap)\n",
    "# Keep test set zero-shot-like\n",
    "#Why 8? Why not. Median of sents per sense is 14\n",
    "# just experimenting with test size split till min 2 sents per sense is achieved\n",
    "\n",
    "#validation set\n",
    "#from remaining examples, applies sense freq filter (8) to keep train covering all labels\n",
    "#filter for combination compliance\n",
    "#sample 4 examples from all\n",
    "\n",
    "def split_compliance(df):\n",
    "    at_least_two = df.groupby(\"lemma\").senseID.nunique().min() >= 2\n",
    "    sent_num_cond = df.groupby(\"senseID\").sent.count().min() >= 2\n",
    "    print(\"two sense per lemma: {}\".format(at_least_two))\n",
    "    print(\"two examples per sense: {}\".format(sent_num_cond))\n",
    "    \n",
    "_ , test_df = train_test_split(clean_df, test_size= 0.6, random_state=101, stratify = clean_df[\"senseID\"])\n",
    "#print(test_df.groupby(\"senseID\").sent.count().describe())\n",
    "\n",
    "over_eight = test_df.groupby(\"senseID\").filter(lambda x: x.sent.count() >= 8).copy()\n",
    "capped_senses = over_eight.groupby(\"senseID\", group_keys=False).apply(lambda x: x.sample(n = 8, random_state = 101))\n",
    "low_senses = test_df.drop(over_eight.index) # Only keep low ones\n",
    "\n",
    "test_df = pd.concat([low_senses, capped_senses])\n",
    "remain_df = clean_df.drop(test_df.index)\n",
    "\n",
    "proto_val = remain_df.groupby(\"senseID\").filter(lambda x: x.sent.count() >= 8).copy()\n",
    "proto_val = elim_single_senses(proto_val)\n",
    "proto_val = elim_too_few_senses(proto_val)\n",
    "val_df = proto_val.groupby(\"senseID\", group_keys=False).apply(lambda x: x.sample(n = 4, random_state = 101))\n",
    "train_df = remain_df.drop(val_df.index)\n",
    "\n",
    "split_compliance(train_df)\n",
    "split_compliance(val_df)\n",
    "split_compliance(test_df)\n",
    "\n",
    "filter_impact(train_df, \"train\")\n",
    "filter_impact(val_df, \"val\")\n",
    "filter_impact(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55fede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combo experiments: \n",
    "#- save intermediate results in a separate folder\n",
    "#- make it easy to start from some point\n",
    "#change sampling strategy to just take 50% neg pairs?\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "import math\n",
    "\n",
    "save_intermed = \"wsd_data/combo_train_staging/\"\n",
    "column_format = [\"lemma\", \"text\", \"label\", \"senseID1\", \"senseID2\", \"sentID1\", \"sentID2\"]\n",
    "\n",
    "def df_to_combos(df):\n",
    "    #breaks down df into row combinations\n",
    "    #transforms each combination df into a new row\n",
    "    #only used for positive examples because different approach needed for neg\n",
    "    gather = [df.loc[index,:] for index in list(combinations(df.index,2))]\n",
    "    return gather\n",
    "    \n",
    "def mod_rows(pair_df, label):\n",
    "    example1, example2 = [body for ind, body in pair_df.iterrows()]\n",
    "    txt_pairs = example1[\"sent\"] + \" [SEP] \" + example2[\"sent\"]\n",
    "    out_vals = [example1[\"lemma\"], txt_pairs, label, \\\n",
    "                example1[\"senseID\"], example2[\"senseID\"], example1[\"sentID\"], example2[\"sentID\"]]\n",
    "    out_index = [\"lemma\", \"text\", \"label\", \"senseID1\", \"senseID2\", \"sentID1\", \"sentID2\"]\n",
    "    return pd.DataFrame([out_vals], columns = out_index)\n",
    "\n",
    "def harvest_positive_pairs(sense_df):\n",
    "    #takes a df of a single sense and combines the examples for positive\n",
    "    df_with_two =  df_to_combos(sense_df)\n",
    "    df_list_ones = [mod_rows(i, 1) for i in df_with_two]\n",
    "    positive_pairs_df = pd.concat(df_list_ones).reset_index(drop = True)\n",
    "    return positive_pairs_df\n",
    "    \n",
    "#generating negative combos\n",
    "#positives can be generates wihtin a senseID groupby no problem\n",
    "#negatives need access on a lemma level, so maybe it's two different loops actually\n",
    "\n",
    "def get_neg_goal(df):\n",
    "    #takes in a lemma-level sub_df\n",
    "    #returns sent_num per sense, positive combinations and required sents to be sampled for the neg\n",
    "    sense_list = list(df.senseID.unique())\n",
    "    sent_counts = df.groupby(\"senseID\").sent.nunique()\n",
    "    sent_counts = [sent_counts[sense] for sense in sense_list]\n",
    "    positive_combo_n = [math.comb(sent_count, 2) for sent_count in sent_counts]\n",
    "    required_negatives = [math.ceil(pos_combo/pos_example) for pos_combo, pos_example in zip(positive_combo_n, sent_counts)]\n",
    "    #sent_counts, positive_combo_n, \n",
    "    return required_negatives\n",
    "\n",
    "def check_add(candidate_pair, save_df, keytracker):\n",
    "    #checks if the combo is already present in a neg collector df\n",
    "    #if not, it adds it to the neg storage and the keytracker\n",
    "    #it modifies the objects passed into args, doesn't return anything\n",
    "    pair_id = candidate_pair[[\"sentID1\", \"sentID2\"]].values\n",
    "    pair_id = frozenset(pair_id[0])\n",
    "    if pair_id not in keytracker:\n",
    "        keytracker.add(pair_id)\n",
    "        save_df.loc[len(save_df.index)] = candidate_pair.values[0]\n",
    "\n",
    "def combine_pos_neg(df_pos, df_neg, save_df, keytracker):\n",
    "    #will generate non-matching examples into a df format \n",
    "    src = product(df_pos.values, df_neg.values)\n",
    "    for example1, example2 in src:\n",
    "        row_values = list(example1) + list(example2)\n",
    "        lemma = row_values[0]\n",
    "        sent1 = row_values[3]\n",
    "        sent2 = row_values[10]\n",
    "        txt_pair = sent1 + \" [SEP] \" + sent2\n",
    "        senseID1 = row_values[4]\n",
    "        sentID1 = row_values[6]\n",
    "        senseID2 = row_values[11]\n",
    "        sentID2 = row_values[13]\n",
    "        row_data = [lemma, txt_pair, 0, senseID1, senseID2, sentID1, sentID2]\n",
    "        temp_neg = pd.DataFrame([row_data], columns =  [\"lemma\", \"text\", \"label\", \"senseID1\", \"senseID2\", \"sentID1\", \"sentID2\"])\n",
    "        check_add(temp_neg, save_df, keytracker)\n",
    "    \n",
    "def harvest_negative_pairs(df):\n",
    "    #works on lemma level\n",
    "    #get remainder without i-th sense and sample required negative sents\n",
    "    collector = pd.DataFrame([], columns =  [\"lemma\", \"text\", \"label\", \"senseID1\", \"senseID2\", \"sentID1\", \"sentID2\"])\n",
    "    keys = set()\n",
    "    required_negatives = get_neg_goal(df)\n",
    "    for senseID in range(df.senseID.nunique()):        \n",
    "        anti_df = df[df[\"senseID\"].apply(lambda x: x.split(\"%\")[1] != str(senseID))]\n",
    "        samp_target = required_negatives[senseID] \\\n",
    "                if anti_df.sent.nunique() > required_negatives[senseID] else anti_df.sent.nunique()    \n",
    "        anti_samples = anti_df.sample(random_state = 101, n = samp_target)\n",
    "        pos_samples = df[df[\"senseID\"].apply(lambda x: x.split(\"%\")[1] == str(senseID))]\n",
    "        combine_pos_neg(pos_samples, anti_samples, collector, keys)\n",
    "    return collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "039d5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop for pair generation over the train_df\n",
    "\"\"\"\n",
    "lemma_groups = train_df.sort_values('lemma').groupby(\"lemma\")\n",
    "for lemma, lemma_group in tqdm(lemma_groups):\n",
    "    positive_lemma = [harvest_positive_pairs(sense) for _, sense in lemma_group.groupby(\"senseID\")]\n",
    "    pos_pairs = pd.concat(positive_lemma)\n",
    "    neg_pairs = harvest_negative_pairs(lemma_group)\n",
    "    joined = pd.concat([pos_pairs, neg_pairs])\n",
    "    joined.to_csv(save_intermed + \"{}.csv\".format(lemma))\n",
    "\"\"\"\n",
    "\n",
    "#loading the temp files into a unified df\n",
    "\"\"\"\n",
    "def mass_load(file_list):\n",
    "    df_collection = [pd.read_csv(save_intermed + \"/\" + i) for i in file_list if \".csv\" in i]\n",
    "    return pd.concat(df_collection)\n",
    "combo_df = mass_load(file_list)\n",
    "\"\"\"\n",
    "\n",
    "#uniform index (senseID1xsenseID2) for combo df\n",
    "def senseID_order(senseID1, senseID2):\n",
    "    id1, id2 = [i.split(\"%\")[1] for i in [senseID1, senseID2]]\n",
    "    if int(id2) >= int(id1):\n",
    "        return \"{}_{}\".format(senseID1, senseID2)\n",
    "    else:\n",
    "        return \"{}_{}\".format(senseID2, senseID1)\n",
    "\n",
    "combo_df[\"senseID2x\"] = combo_df.apply(lambda x: senseID_order(x.senseID1, x.senseID2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3422f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sampling 10% and 20% train datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "combo_big = pd.read_csv(\"wsd_data/combo_dfs/combo_ready/combo_train_fin.csv\")\n",
    "prop = 0.2 #required % of the main combo df\n",
    "\n",
    "target_size = len(combo_big)*prop\n",
    "keeper = combo_big.groupby(\"senseID2x\").filter(lambda x: x.text.count() == 1)\n",
    "target_prop = (target_size - len(keeper))/3683255\n",
    "\n",
    "big_wo_keeper = combo_big.drop(keeper.index)\n",
    "_, combo_20 = train_test_split(big_wo_keeper, stratify = big_wo_keeper[[\"senseID2x\"]], test_size = target_prop, random_state=101)\n",
    "\n",
    "df_20 = pd.concat([keeper, combo_20])\n",
    "#df_20.to_csv(\"wsd_data/combo_dfs/combo_ready/combo_train_20s.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c34b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#balancing (non)-matching sent pairs\n",
    "import pandas as pd\n",
    "pth = \"/home/fijavzz/workspace/wsd_data/combo_dfs/\"\n",
    "\n",
    "val_df = pd.read_csv(pth + 'combo_val_df.csv')\n",
    "train_df = pd.read_csv(pth + 'combo_train.csv')\n",
    "test_df = pd.read_csv(pth + 'combo_test_df.csv')\n",
    "train_mini_df = pd.read_csv(pth + 'combo_train_mini.csv')\n",
    "\n",
    "def senseID_order(senseID1, senseID2):\n",
    "    id1, id2 = [i.split(\"%\")[1] for i in [senseID1, senseID2]]\n",
    "    if int(id2) >= int(id1):\n",
    "        return \"{}_{}\".format(senseID1, senseID2)\n",
    "    else:\n",
    "        return \"{}_{}\".format(senseID2, senseID1)\n",
    "\n",
    "val_df[\"senseID2x\"] = val_df.apply(lambda x: senseID_order(x.sense1, x.sense2), axis = 1)\n",
    "test_df[\"senseID2x\"] = test_df.apply(lambda x: senseID_order(x.sense1, x.sense2), axis = 1)\n",
    "\n",
    "def sep_sents(df, label): #text for train, \"text_pair\" for rest\n",
    "    split = df[label].apply(lambda x: x.split('[SEP]'))\n",
    "    df[\"sent1\"] = split.apply(lambda x: x[0])\n",
    "    df[\"sent2\"] = split.apply(lambda x: x[1])\n",
    "    return df\n",
    "\n",
    "val_df = sep_sents(val_df, \"text_pair\")\n",
    "test_df = sep_sents(test_df, \"text_pair\")\n",
    "train_df = sep_sents(train_df, \"text\")\n",
    "\n",
    "def balance_plz(df):\n",
    "    #downsamples examples with negative class\n",
    "    pos = df[df.label == 1].copy()\n",
    "    neg = df[df.label == 0].copy()\n",
    "    req_prop = round(len(pos)/len(neg), 3)\n",
    "    neg_down = neg.groupby(\"senseID2x\", group_keys = False).apply(lambda x: x.sample(frac = req_prop, random_state=101))\n",
    "    return pd.concat([pos, neg_down])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative approach for sentence combination dfs\n",
    "#works for smaller dfs (test, val)\n",
    "#abandoned for lack of control and long computation time for train df\n",
    "### see solution above with intermittent saving after each lemma and limited negative examples\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def sent_combos(target_df):\n",
    "  #pobere vse kombinacije v df-ju. lahko traja kar dolgo, da zaključi.\n",
    "  pair_df = pd.DataFrame(columns=['text_pair','sense1','sense2', \"sent1_ind\", \"sent2_ind\", \"lemma\",'label'])\n",
    "  sense_groups = target_df.groupby(\"lemma\")\n",
    "  for lemma, body in tqdm(sense_groups):\n",
    "    tupl = [(sent, sense, sent_ind) for sent, sense, sent_ind in zip(body.sent, body.senseID, body.sentID)]\n",
    "    combos = [i for i in combinations(tupl, 2)]\n",
    "    for pair in combos:\n",
    "      pack1, pack2 = pair\n",
    "      t1, sense1, sent_ind1 = pack1\n",
    "      t2, sense2, sent_ind2 = pack2\n",
    "      text_pair = t1 + \" [SEP] \" + t2\n",
    "      label = 1 if (sense1 == sense2) else 0\n",
    "      pair_df.loc[len(pair_df.index)] = [text_pair, sense1, sense2, sent_ind1, sent_ind2, lemma, label] \n",
    "  return pair_df\n",
    "\n",
    "\"\"\"\n",
    "test_combos = sent_combos(test_df)\n",
    "val_combos = sent_combos(val_df)\n",
    "val_combos.to_csv(\"wsd_data/combo_val_df.csv\", index = False)\n",
    "train_combos = sent_combos(train_df)\n",
    "train_combos.to_csv(\"wsd_data/combo_train_df.csv\", index = False)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dse_env",
   "language": "python",
   "name": "dse_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
